# ğŸ¤– Small Language Model from Scratch
**Build and train your own GPT-style transformer from scratch in PyTorch**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Yashjain0099/Small_Language_Model_from_Scratch/blob/main/SLM_(scratch).ipynb)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)

---

## ğŸ“‹ Table of Contents
- [ğŸ¯ What This Is](#-what-this-is)
- [âœ¨ Key Features](#-key-features)
- [ğŸ—ï¸ How It Works](#ï¸-how-it-works)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ’» Usage Examples](#-usage-examples)
- [ğŸ“Š Results](#-results)
- [ğŸ› ï¸ Project Structure](#ï¸-project-structure)
- [ğŸ¤ Contributing](#-contributing)

---

## ğŸ¯ What This Is

A **complete GPT-style language model** built entirely from scratch using PyTorch. No black boxes - every component is implemented and explained.

**Perfect for:**
- ğŸ“ Learning how transformers actually work
- ğŸ”¬ Experimenting with language model architectures  
- ğŸš€ Building your own AI text generation projects

---

## âœ¨ Key Features

| Feature | Description | Status |
|---------|-------------|---------|
| ğŸ§  **Custom Transformer** | Multi-head attention, feed-forward networks, layer norm | âœ… Complete |
| ğŸ”¤ **Smart Tokenization** | GPT-2 BPE tokenizer via tiktoken | âœ… Complete |
| âš¡ **Fast Training** | Mixed precision, gradient accumulation, CUDA support | âœ… Complete |
| ğŸ¨ **Text Generation** | Temperature sampling, top-k filtering | âœ… Complete |
| â˜ï¸ **Colab Ready** | One-click deployment in Google Colab | âœ… Complete |

---

## ğŸ—ï¸ How It Works

```mermaid
graph LR
    A[ğŸ“ Text Input] --> B[ğŸ”¤ Tokenizer]
    B --> C[ğŸ§® Embeddings]
    C --> D[ğŸ”„ Transformer Blocks]
    D --> E[ğŸ¯ Output Layer]
    E --> F[ğŸ“– Generated Text]
    
    subgraph "ğŸ”„ Transformer Block"
        G[ğŸ¯ Self-Attention] --> H[â• Add & Norm]
        H --> I[ğŸ§  Feed Forward]
        I --> J[â• Add & Norm]
    end
    
    style A fill:#e3f2fd
    style F fill:#e8f5e8
    style G fill:#fff3e0
    style I fill:#f3e5f5
```

**The Process:**
1. **Tokenize** text using GPT-2's tokenizer
2. **Embed** tokens and add position information
3. **Transform** through multiple attention layers
4. **Generate** next token predictions
5. **Sample** from predictions to create new text

---

## ğŸš€ Quick Start

### Option 1: Google Colab (Recommended) â˜ï¸
Click the badge above â†’ Run all cells â†’ Start generating text!

### Option 2: Local Setup ğŸ’»
```bash
# Clone the repo
git clone https://github.com/Yashjain0099/Small_Language_Model_from_Scratch.git
cd Small_Language_Model_from_Scratch

# Install dependencies  
pip install torch tiktoken numpy matplotlib tqdm

# Run the notebook
jupyter notebook SLM_\(scratch\).ipynb
```

---

## ğŸ’» Usage Examples

### ğŸ¯ Basic Text Generation
```python
# Load your trained model
model = SmallLanguageModel()
model.load_state_dict(torch.load('model.pth'))

# Generate text
prompt = "A little girl went to the woods"
output = model.generate(prompt, max_length=50, temperature=0.8)
print(output)
```

### ğŸ¨ Creative vs Focused Generation
```python
# Creative mode (higher temperature)
creative = model.generate("Once upon a time", temperature=1.2, top_k=50)

# Focused mode (lower temperature)  
focused = model.generate("The capital of France is", temperature=0.3, top_k=10)
```

### ğŸ”§ Training Your Own Model
```python
# Quick training setup
trainer = LanguageModelTrainer(model)
trainer.train(
    data_path="your_text_data.txt",
    batch_size=16,
    learning_rate=3e-4,
    epochs=5
)
```

---

## ğŸ“Š Results

### ğŸ“ Sample Generation
**Input:** *"A little girl went to the woods"*

**Output:**
```
A little girl went to the woods and he was looking at the animals and he saw 
a little boy with a big smile on its face. He knew she would never bring 
medicine before. One day, the girl called Jeff went for a walk...
```

### ğŸ“ˆ Training Progress
![Training Loss](images/training_progress.png)
*Model learns to predict text better over time*

### âš¡ Performance Stats
- **Model Size:** ~25M parameters
- **Training Time:** ~2 hours on GPU
- **Generation Speed:** 50+ tokens/second
- **Memory Usage:** <4GB GPU memory

---

## ğŸ› ï¸ Project Structure

```
ğŸ“¦ Small-Language-Model
â”œâ”€â”€ ğŸ“œ SLM_(scratch).ipynb    # Main notebook with everything
â”œâ”€â”€ ğŸ“„ README.md              # This file
â””â”€â”€ ğŸ“ images/                # Screenshots and plots
    â”œâ”€â”€ training_progress.png
    â”œâ”€â”€ model_architecture.png
    â””â”€â”€ generation_examples.png
```

### ğŸ§© Code Components

| Component | What It Does | Lines of Code |
|-----------|--------------|---------------|
| **Tokenizer** | Converts text â†” numbers | ~50 lines |
| **Model** | Transformer architecture | ~200 lines |
| **Training** | Loss calculation & optimization | ~100 lines |
| **Generation** | Text sampling & generation | ~80 lines |

---

## ğŸ“ What You'll Learn

### ğŸ” Core Concepts
- âœ… How self-attention actually works
- âœ… Why transformers are so powerful
- âœ… How language models generate text
- âœ… Modern training techniques (mixed precision, scheduling)

### ğŸ§  Technical Skills
- âœ… Building neural networks from scratch
- âœ… Implementing attention mechanisms
- âœ… Training large models efficiently
- âœ… Text generation and sampling methods

---

## ğŸ¯ Model Configuration

```python
# Default model settings
MODEL_CONFIG = {
    'vocab_size': 50257,      # GPT-2 vocabulary
    'd_model': 512,           # Hidden dimension
    'n_heads': 8,             # Attention heads
    'n_layers': 6,            # Transformer layers
    'max_seq_len': 1024,      # Maximum sequence length
}
```

**Want bigger models?** Just increase the parameters:
- ğŸ“± **Tiny:** 256 dim, 4 heads, 4 layers (~6M params)
- ğŸ–¥ï¸ **Small:** 512 dim, 8 heads, 6 layers (~25M params)  
- ğŸš€ **Medium:** 768 dim, 12 heads, 12 layers (~85M params)

---

## ğŸ”§ Advanced Features

### âš¡ Training Optimizations
- **Mixed Precision:** 2x faster training
- **Gradient Accumulation:** Larger effective batch sizes
- **Learning Rate Scheduling:** Warmup + cosine decay
- **Checkpointing:** Resume training anytime

### ğŸ¨ Generation Options
- **Temperature:** Control randomness (0.1 = boring, 1.5 = wild)
- **Top-K:** Only sample from K most likely tokens
- **Max Length:** Control output length
- **Repetition Penalty:** Avoid repetitive text

---

## ğŸ¤ Contributing

Found a bug? Want to add features? Contributions welcome!

1. **Fork** the repo
2. **Create** a feature branch
3. **Make** your changes  
4. **Submit** a pull request

**Ideas for contributions:**
- ğŸ¯ Different attention mechanisms
- ğŸ“Š Better evaluation metrics
- ğŸ¨ New generation techniques
- ğŸ“š More example datasets
- ğŸ› Bug fixes and improvements

---

## ğŸ“š Learn More

### ğŸ“– Helpful Resources
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual explanation
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original paper
- [Let's Build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) - Andrej Karpathy's tutorial

### ğŸ¬ Video Walkthrough
*Coming soon: Full video explanation of the code!*

---

## ğŸ“ Contact

**Yash Jain**
- ğŸ™ GitHub: [@Yashjain0099](https://github.com/Yashjain0099)
- ğŸ“§ Questions? Open an issue!

---

## â­ Show Your Support

If this helped you understand language models:
- â­ **Star** the repository
- ğŸ´ **Fork** for your experiments
- ğŸ“¢ **Share** with friends
- ğŸ› **Report** bugs you find

---

**Built with â¤ï¸ for learning and understanding AI**

*Ready to dive in? Click the Colab badge and start experimenting!* ğŸš€
